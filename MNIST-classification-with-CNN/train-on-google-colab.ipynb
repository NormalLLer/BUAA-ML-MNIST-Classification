{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Classification With CNN\n",
    "> This file was downloaded from Google Colab"
   ],
   "metadata": {
    "id": "JqFH6oo9Jhgo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Since there is no downloaded MNIST dataset on Google Colab, \n",
    "the code in this cell is used to download the dataset directly \n",
    "from `http://yann.lecun.com/exdb/mnist` through python code \n",
    "and decompress it in the current directory. After this code \n",
    "is executed, the code of the following cell can be executed \n",
    "to train the CNN model.\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Download the MNIST dataset files\n",
    "def download_mnist(url, filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Extract the MNIST dataset files\n",
    "def extract_mnist(filename, save_dir):\n",
    "    with gzip.open(filename, 'rb') as f_in:\n",
    "        with open(save_dir, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "# Load MNIST image data\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Skip the file header\n",
    "        f.read(16)\n",
    "        # Read the image data\n",
    "        buf = f.read()\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).reshape(-1, 28, 28)\n",
    "    return data\n",
    "\n",
    "# Load MNIST label data\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Skip the file header\n",
    "        f.read(8)\n",
    "        # Read the label data\n",
    "        buf = f.read()\n",
    "        data = np.frombuffer(buf, dtype=np.uint8)\n",
    "    return data\n",
    "\n",
    "\n",
    "os.makedirs(\"./data\")\n",
    "\n",
    "# Download MNIST dataset files\n",
    "download_mnist('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', './data/train-images-idx3-ubyte.gz')\n",
    "download_mnist('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', './data/train-labels-idx1-ubyte.gz')\n",
    "download_mnist('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', './data/t10k-images-idx3-ubyte.gz')\n",
    "download_mnist('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', './data/t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "# Extract MNIST dataset files\n",
    "extract_mnist('./data/train-images-idx3-ubyte.gz', './data/train-images.idx3-ubyte')\n",
    "extract_mnist('./data/train-labels-idx1-ubyte.gz', './data/train-labels.idx1-ubyte')\n",
    "extract_mnist('./data/t10k-images-idx3-ubyte.gz', './data/t10k-images.idx3-ubyte')\n",
    "extract_mnist('./data/t10k-labels-idx1-ubyte.gz', './data/t10k-labels.idx1-ubyte')\n",
    "\n",
    "# Load training and test set data\n",
    "train_images = load_mnist_images('./data/train-images.idx3-ubyte')\n",
    "train_labels = load_mnist_labels('./data/train-labels.idx1-ubyte')\n",
    "test_images = load_mnist_images('./data/t10k-images.idx3-ubyte')\n",
    "test_labels = load_mnist_labels('./data/t10k-labels.idx1-ubyte')\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIzMnFB8HOBh",
    "outputId": "33954d73-b759-4a56-804b-2743b2717e86"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training images shape: (60000, 28, 28)\n",
      "Training labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "id": "Wa5fO2LJwNb2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "57ba6e9d-78f9-4732-8eeb-61b129021b88"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training: \n",
      "Epoch [1/13], Step [100/469], Loss: 1.4851, Learning Rate: 0.0010000000\n",
      "Epoch [1/13], Step [200/469], Loss: 1.4900, Learning Rate: 0.0010000000\n",
      "Epoch [1/13], Step [300/469], Loss: 1.4912, Learning Rate: 0.0010000000\n",
      "Epoch [1/13], Step [400/469], Loss: 1.4856, Learning Rate: 0.0010000000\n",
      "Epoch [2/13], Step [100/469], Loss: 1.4691, Learning Rate: 0.0010000000\n",
      "Epoch [2/13], Step [200/469], Loss: 1.4921, Learning Rate: 0.0010000000\n",
      "Epoch [2/13], Step [300/469], Loss: 1.4797, Learning Rate: 0.0010000000\n",
      "Epoch [2/13], Step [400/469], Loss: 1.4779, Learning Rate: 0.0010000000\n",
      "Epoch [3/13], Step [100/469], Loss: 1.4694, Learning Rate: 0.0001000000\n",
      "Epoch [3/13], Step [200/469], Loss: 1.4713, Learning Rate: 0.0001000000\n",
      "Epoch [3/13], Step [300/469], Loss: 1.4666, Learning Rate: 0.0001000000\n",
      "Epoch [3/13], Step [400/469], Loss: 1.4848, Learning Rate: 0.0001000000\n",
      "Epoch [4/13], Step [100/469], Loss: 1.4615, Learning Rate: 0.0001000000\n",
      "Epoch [4/13], Step [200/469], Loss: 1.4764, Learning Rate: 0.0001000000\n",
      "Epoch [4/13], Step [300/469], Loss: 1.4612, Learning Rate: 0.0001000000\n",
      "Epoch [4/13], Step [400/469], Loss: 1.4612, Learning Rate: 0.0001000000\n",
      "Epoch [5/13], Step [100/469], Loss: 1.4612, Learning Rate: 0.0000100000\n",
      "Epoch [5/13], Step [200/469], Loss: 1.4679, Learning Rate: 0.0000100000\n",
      "Epoch [5/13], Step [300/469], Loss: 1.4612, Learning Rate: 0.0000100000\n",
      "Epoch [5/13], Step [400/469], Loss: 1.4612, Learning Rate: 0.0000100000\n",
      "Epoch [6/13], Step [100/469], Loss: 1.4688, Learning Rate: 0.0000100000\n",
      "Epoch [6/13], Step [200/469], Loss: 1.4690, Learning Rate: 0.0000100000\n",
      "Epoch [6/13], Step [300/469], Loss: 1.4686, Learning Rate: 0.0000100000\n",
      "Epoch [6/13], Step [400/469], Loss: 1.4612, Learning Rate: 0.0000100000\n",
      "Epoch [7/13], Step [100/469], Loss: 1.4765, Learning Rate: 0.0000010000\n",
      "Epoch [7/13], Step [200/469], Loss: 1.4693, Learning Rate: 0.0000010000\n",
      "Epoch [7/13], Step [300/469], Loss: 1.4637, Learning Rate: 0.0000010000\n",
      "Epoch [7/13], Step [400/469], Loss: 1.4612, Learning Rate: 0.0000010000\n",
      "Epoch [8/13], Step [100/469], Loss: 1.4620, Learning Rate: 0.0000010000\n",
      "Epoch [8/13], Step [200/469], Loss: 1.4618, Learning Rate: 0.0000010000\n",
      "Epoch [8/13], Step [300/469], Loss: 1.4612, Learning Rate: 0.0000010000\n",
      "Epoch [8/13], Step [400/469], Loss: 1.4613, Learning Rate: 0.0000010000\n",
      "Epoch [9/13], Step [100/469], Loss: 1.4612, Learning Rate: 0.0000001000\n",
      "Epoch [9/13], Step [200/469], Loss: 1.4612, Learning Rate: 0.0000001000\n",
      "Epoch [9/13], Step [300/469], Loss: 1.4615, Learning Rate: 0.0000001000\n",
      "Epoch [9/13], Step [400/469], Loss: 1.4690, Learning Rate: 0.0000001000\n",
      "Epoch [10/13], Step [100/469], Loss: 1.4686, Learning Rate: 0.0000001000\n",
      "Epoch [10/13], Step [200/469], Loss: 1.4693, Learning Rate: 0.0000001000\n",
      "Epoch [10/13], Step [300/469], Loss: 1.4668, Learning Rate: 0.0000001000\n",
      "Epoch [10/13], Step [400/469], Loss: 1.4612, Learning Rate: 0.0000001000\n",
      "Epoch [11/13], Step [100/469], Loss: 1.4689, Learning Rate: 0.0000000100\n",
      "Epoch [11/13], Step [200/469], Loss: 1.4633, Learning Rate: 0.0000000100\n",
      "Epoch [11/13], Step [300/469], Loss: 1.4612, Learning Rate: 0.0000000100\n",
      "Epoch [11/13], Step [400/469], Loss: 1.4690, Learning Rate: 0.0000000100\n",
      "Epoch [12/13], Step [100/469], Loss: 1.4640, Learning Rate: 0.0000000100\n",
      "Epoch [12/13], Step [200/469], Loss: 1.4612, Learning Rate: 0.0000000100\n",
      "Epoch [12/13], Step [300/469], Loss: 1.4692, Learning Rate: 0.0000000100\n",
      "Epoch [12/13], Step [400/469], Loss: 1.4616, Learning Rate: 0.0000000100\n",
      "Epoch [13/13], Step [100/469], Loss: 1.4676, Learning Rate: 0.0000000010\n",
      "Epoch [13/13], Step [200/469], Loss: 1.4689, Learning Rate: 0.0000000010\n",
      "Epoch [13/13], Step [300/469], Loss: 1.4619, Learning Rate: 0.0000000010\n",
      "Epoch [13/13], Step [400/469], Loss: 1.4612, Learning Rate: 0.0000000010\n",
      "\n",
      "Accuracy on test set: 99.55 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "IMAGE_SIZE = 28 * 28  # MNIST image size\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "\n",
    "    \"\"\"\n",
    "    'train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
    "    't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz'\n",
    "    Before use, you need to download the above four files to the `path` directory and unzip them\n",
    "    \"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "\n",
    "    with open(labels_path, 'rb') as label_file:\n",
    "        labels = np.frombuffer(label_file.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with open(images_path, 'rb') as image_file:\n",
    "        images = np.frombuffer(image_file.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), IMAGE_SIZE)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.maxpool5 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear1 = nn.Linear(256, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.linear2 = nn.Linear(512, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.bn4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        x = self.bn5(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu6(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(train_loader, num_epochs, optimizer, scheduler, device):\n",
    "    total_step = len(train_loader)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward prop\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # print results every 100 steps\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Learning Rate: {:.10f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), scheduler.get_last_lr()[0]))\n",
    "\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('\\nAccuracy on test set: {:.2f} %'.format(100 * correct / total))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = './data'\n",
    "\n",
    "    # set random seed to make the result repeatable\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # hyper parameters\n",
    "    batch_size = 128\n",
    "    num_epochs = 13\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # check if CUDA is available and set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # load train set\n",
    "    train_images, train_labels = load_mnist(data_path, kind='train')\n",
    "    train_images = train_images.reshape(-1, 1, 28, 28)  # reshape the size to match the input of CNN\n",
    "\n",
    "    # load test set\n",
    "    test_images, test_labels = load_mnist(data_path, kind='t10k')\n",
    "    test_images = test_images.reshape(-1, 1, 28, 28)  # reshape the size to match the input of CNN\n",
    "\n",
    "    # numpy to tensor\n",
    "    train_images = torch.from_numpy(train_images.copy()).float()\n",
    "    train_labels = torch.from_numpy(train_labels.copy()).long()\n",
    "    test_images = torch.from_numpy(test_images.copy()).float()\n",
    "    test_labels = torch.from_numpy(test_labels.copy()).long()\n",
    "\n",
    "    # create dataset objects\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_images, test_labels)\n",
    "\n",
    "    # create dataset loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # initialize model and loss fn\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=469 * 2, gamma=0.1)\n",
    "\n",
    "    # train\n",
    "    print(\"\\nTraining: \")\n",
    "    train(\n",
    "        train_loader=train_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "    # test\n",
    "    test(\n",
    "        test_loader=test_loader,\n",
    "        model=model,\n",
    "        device=device\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
